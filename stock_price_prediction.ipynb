{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d930120",
   "metadata": {},
   "source": [
    "# Stock Price Prediction Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 1) Imports & Global Settings\n",
    "# ===============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(\"Step 1 completed: libraries imported and global settings configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 2) User Configuration\n",
    "# ===============================================================\n",
    "TICKER = \"PLTR\"\n",
    "LOOKBACK = 60\n",
    "FORECAST_HORIZON = 90\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.2  # used for early stopping only\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "\n",
    "# New options\n",
    "USE_CNN = True\n",
    "USE_BIDIR = False\n",
    "PREDICT_RETURNS = True\n",
    "FORECAST_WEIGHT_GAMMA = 0.02\n",
    "DROPOUT_RATE = 0.1\n",
    "LR = 1e-3\n",
    "LOSS_WEIGHTS = {\"direction\": 1.0, \"forecast\": 0.01}\n",
    "RUN_WALK_FORWARD = False  # optional walk-forward evaluation\n",
    "\n",
    "print(\"Step 2 completed: configuration set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcde90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 3) Download & Feature Engineering\n",
    "# ===============================================================\n",
    "def download_and_engineer(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"Download historical data and compute technical indicators.\"\"\"\n",
    "    df = yf.download(ticker, period=\"5y\")\n",
    "    df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "\n",
    "    # Moving Averages\n",
    "    df[\"MA7\"] = df[\"Close\"].rolling(7).mean()\n",
    "    df[\"MA21\"] = df[\"Close\"].rolling(21).mean()\n",
    "\n",
    "    # MACD\n",
    "    df[\"EMA12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"EMA26\"] = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = df[\"EMA12\"] - df[\"EMA26\"]\n",
    "    df[\"MACD_SIGNAL\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df[\"MA20\"] = df[\"Close\"].rolling(20).mean()\n",
    "    df[\"BB_UPPER\"] = df[\"MA20\"] + 2 * df[\"Close\"].rolling(20).std()\n",
    "    df[\"BB_LOWER\"] = df[\"MA20\"] - 2 * df[\"Close\"].rolling(20).std()\n",
    "\n",
    "    # ATR\n",
    "    prev_close = df[\"Close\"].shift(1)\n",
    "    tr = pd.concat(\n",
    "        [\n",
    "            (df[\"High\"] - df[\"Low\"]).abs(),\n",
    "            (df[\"High\"] - prev_close).abs(),\n",
    "            (df[\"Low\"] - prev_close).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"ATR14\"] = tr.rolling(14).mean()\n",
    "\n",
    "    # Returns, lags and volatility\n",
    "    df[\"RET1\"] = df[\"Close\"].pct_change()\n",
    "    df[\"LOGRET1\"] = np.log1p(df[\"RET1\"])\n",
    "    df[\"RET5\"] = df[\"Close\"].pct_change(5)\n",
    "    df[\"VOLAT_21\"] = df[\"LOGRET1\"].rolling(21).std()\n",
    "    df[\"CLOSE_LAG1\"] = df[\"Close\"].shift(1)\n",
    "    df[\"CLOSE_LAG5\"] = df[\"Close\"].shift(5)\n",
    "\n",
    "    # RSI\n",
    "    delta = df[\"Close\"].diff()\n",
    "    up = delta.clip(lower=0).rolling(14).mean()\n",
    "    down = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    rs = up / down\n",
    "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(f\"{ticker}_features.csv\")\n",
    "    print(f\"Saved feature dataset to {ticker}_features.csv\")\n",
    "    return df\n",
    "\n",
    "feature_cols = [\n",
    "    \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"MA7\", \"MA21\",\n",
    "    \"EMA12\", \"EMA26\", \"MACD\", \"MACD_SIGNAL\", \"MA20\", \"BB_UPPER\",\n",
    "    \"BB_LOWER\", \"ATR14\", \"RET1\", \"LOGRET1\", \"RET5\", \"VOLAT_21\",\n",
    "    \"CLOSE_LAG1\", \"CLOSE_LAG5\", \"RSI\",\n",
    "]\n",
    "\n",
    "df = download_and_engineer(TICKER)\n",
    "print(f\"Step 3 completed: data shape {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47df3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 4) Prepare Data & Build Datasets\n",
    "# ===============================================================\n",
    "def prepare_data(df: pd.DataFrame):\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    val_df = df.iloc[split_idx - LOOKBACK :]\n",
    "    feature_scaler = MinMaxScaler().fit(train_df[feature_cols])\n",
    "    price_scaler = MinMaxScaler().fit(train_df[[\"Close\"]])\n",
    "    train_scaled = feature_scaler.transform(train_df[feature_cols])\n",
    "    val_scaled = feature_scaler.transform(val_df[feature_cols])\n",
    "    return (\n",
    "        train_scaled,\n",
    "        val_scaled,\n",
    "        train_df.reset_index(drop=True),\n",
    "        val_df.reset_index(drop=True),\n",
    "        price_scaler,\n",
    "    )\n",
    "\n",
    "def create_sequences(\n",
    "    scaled_features: np.ndarray,\n",
    "    raw_df: pd.DataFrame,\n",
    "    lookback: int,\n",
    "    horizon: int,\n",
    "    predict_returns: bool,\n",
    "    price_scaler: MinMaxScaler,\n",
    "):\n",
    "    X, y_dir, y_fore = [], [], []\n",
    "    for i in range(lookback, len(scaled_features) - horizon):\n",
    "        X.append(scaled_features[i - lookback : i])\n",
    "        next_logret = raw_df[\"LOGRET1\"].iloc[i]\n",
    "        y_dir.append([1 if next_logret > 0 else 0])\n",
    "        if predict_returns:\n",
    "            y_fore.append(raw_df[\"LOGRET1\"].iloc[i : i + horizon].to_numpy())\n",
    "        else:\n",
    "            scaled_prices = price_scaler.transform(raw_df[[\"Close\"]].iloc[i : i + horizon])\n",
    "            y_fore.append(scaled_prices.flatten())\n",
    "    return np.array(X), np.array(y_dir), np.array(y_fore)\n",
    "\n",
    "def build_datasets(df: pd.DataFrame):\n",
    "    train_scaled, val_scaled, train_df, val_df, price_scaler = prepare_data(df)\n",
    "    X_train, y_dir_train, y_fore_train = create_sequences(\n",
    "        train_scaled, train_df, LOOKBACK, FORECAST_HORIZON, PREDICT_RETURNS, price_scaler\n",
    "    )\n",
    "    X_val, y_dir_val, y_fore_val = create_sequences(\n",
    "        val_scaled, val_df, LOOKBACK, FORECAST_HORIZON, PREDICT_RETURNS, price_scaler\n",
    "    )\n",
    "    return (\n",
    "        X_train, y_dir_train, y_fore_train,\n",
    "        X_val, y_dir_val, y_fore_val,\n",
    "        price_scaler, val_df,\n",
    "    )\n",
    "\n",
    "(X_train, y_dir_train, y_fore_train,\n",
    " X_val, y_dir_val, y_fore_val,\n",
    " price_scaler, val_df) = build_datasets(df)\n",
    "print(f\"Step 4 completed: train samples {X_train.shape[0]}, val samples {X_val.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39129727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 5) Build and Compile Model\n",
    "# ===============================================================\n",
    "def build_model(n_features: int) -> Model:\n",
    "    inp = Input(shape=(LOOKBACK, n_features))\n",
    "    x = inp\n",
    "    if USE_CNN:\n",
    "        x = Conv1D(filters=32, kernel_size=3, padding=\"causal\", activation=\"relu\")(x)\n",
    "    if USE_BIDIR:\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    else:\n",
    "        x = LSTM(128, return_sequences=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    x = LSTM(32, return_sequences=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    dir_out = Dense(1, activation=\"sigmoid\", name=\"direction\")(x)\n",
    "    reg_out = Dense(FORECAST_HORIZON, activation=\"linear\", name=\"forecast\")(x)\n",
    "    model = Model(inp, [dir_out, reg_out])\n",
    "    return model\n",
    "\n",
    "def weighted_mse(horizon: int, gamma: float):\n",
    "    w = K.exp(-gamma * K.arange(0, horizon, dtype=\"float32\"))\n",
    "    w = w / K.sum(w)\n",
    "    def loss(y_true, y_pred):\n",
    "        se = K.square(y_true - y_pred)\n",
    "        return K.sum(se * w, axis=1)\n",
    "    return loss\n",
    "\n",
    "def compile_model(model: Model) -> Model:\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LR),\n",
    "        loss={\n",
    "            \"direction\": \"binary_crossentropy\",\n",
    "            \"forecast\": weighted_mse(FORECAST_HORIZON, FORECAST_WEIGHT_GAMMA),\n",
    "        },\n",
    "        loss_weights=LOSS_WEIGHTS,\n",
    "        metrics={\"direction\": \"accuracy\", \"forecast\": \"mae\"},\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(X_train.shape[2])\n",
    "model = compile_model(model)\n",
    "print(\"Step 5 completed: model built and compiled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 6) Training\n",
    "# ===============================================================\n",
    "def train_model(model: Model, X_train, y_dir_train, y_fore_train, X_val, y_dir_val, y_fore_val):\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=EARLY_STOP_PATIENCE, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        {\"direction\": y_dir_train, \"forecast\": y_fore_train},\n",
    "        validation_data=(X_val, {\"direction\": y_dir_val, \"forecast\": y_fore_val}),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "history = train_model(model, X_train, y_dir_train, y_fore_train, X_val, y_dir_val, y_fore_val)\n",
    "print(\"Step 6 completed: training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# 7) Evaluation, Visualisation & Saving\n",
    "# ===============================================================\n",
    "def logrets_to_prices(start_price: float, logrets: np.ndarray) -> np.ndarray:\n",
    "    cumulative = np.exp(np.cumsum(logrets))\n",
    "    return start_price * cumulative\n",
    "\n",
    "def evaluate(model: Model, X_val, y_dir_val, y_fore_val, price_scaler: MinMaxScaler, val_df: pd.DataFrame):\n",
    "    dir_pred, fore_pred = model.predict(X_val, verbose=0)\n",
    "    dir_bin = (dir_pred > 0.5).astype(int)\n",
    "    dir_acc = accuracy_score(y_dir_val, dir_bin)\n",
    "    print(f\"Direction Accuracy: {dir_acc:.3f}\")\n",
    "    pred_prices, true_prices = [], []\n",
    "    for i in range(len(fore_pred)):\n",
    "        start_price = val_df[\"Close\"].iloc[i + LOOKBACK - 1]\n",
    "        if PREDICT_RETURNS:\n",
    "            pred_path = logrets_to_prices(start_price, fore_pred[i])\n",
    "            true_path = logrets_to_prices(start_price, y_fore_val[i])\n",
    "        else:\n",
    "            pred_path = price_scaler.inverse_transform(fore_pred[i].reshape(-1, 1)).flatten()\n",
    "            true_path = price_scaler.inverse_transform(y_fore_val[i].reshape(-1, 1)).flatten()\n",
    "        pred_prices.append(pred_path)\n",
    "        true_prices.append(true_path)\n",
    "    pred_prices = np.array(pred_prices)\n",
    "    true_prices = np.array(true_prices)\n",
    "    rmse = np.sqrt(mean_squared_error(true_prices.flatten(), pred_prices.flatten()))\n",
    "    mae = mean_absolute_error(true_prices.flatten(), pred_prices.flatten())\n",
    "    r2 = r2_score(true_prices.flatten(), pred_prices.flatten())\n",
    "    print(f\"Forecast RMSE: {rmse:.2f}\")\n",
    "    print(f\"Forecast MAE: {mae:.2f}\")\n",
    "    print(f\"Forecast R2: {r2:.2f}\")\n",
    "    last_close = val_df[\"Close\"].iloc[LOOKBACK - 1 : -(FORECAST_HORIZON)].to_numpy()\n",
    "    baseline_hold = np.repeat(last_close[:, None], FORECAST_HORIZON, axis=1)\n",
    "    mean_logret = val_df[\"LOGRET1\"].iloc[: len(val_df) - FORECAST_HORIZON].mean()\n",
    "    baseline_drift = []\n",
    "    for price in last_close:\n",
    "        drift_path = logrets_to_prices(price, np.full(FORECAST_HORIZON, mean_logret))\n",
    "        baseline_drift.append(drift_path)\n",
    "    baseline_drift = np.array(baseline_drift)\n",
    "    for name, base in {\"Hold\": baseline_hold, \"Drift\": baseline_drift}.items():\n",
    "        b_rmse = np.sqrt(mean_squared_error(true_prices.flatten(), base.flatten()))\n",
    "        b_mae = mean_absolute_error(true_prices.flatten(), base.flatten())\n",
    "        print(f\"{name} Baseline RMSE: {b_rmse:.2f} | MAE: {b_mae:.2f}\")\n",
    "    cm = confusion_matrix(y_dir_val, dir_bin)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Direction Confusion Matrix\")\n",
    "    plt.show()\n",
    "    return pred_prices, true_prices\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    if \"direction_loss\" in history.history:\n",
    "        plt.plot(history.history[\"direction_loss\"], label=\"Dir Loss\")\n",
    "        plt.plot(history.history[\"val_direction_loss\"], label=\"Val Dir Loss\")\n",
    "    if \"forecast_loss\" in history.history:\n",
    "        plt.plot(history.history[\"forecast_loss\"], label=\"Fore Loss\")\n",
    "        plt.plot(history.history[\"val_forecast_loss\"], label=\"Val Fore Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training History\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(pred_prices, true_prices):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for i in range(min(5, len(pred_prices))):\n",
    "        plt.plot(true_prices[i], color=\"blue\", alpha=0.3, label=\"Actual\" if i == 0 else \"\")\n",
    "        plt.plot(pred_prices[i], color=\"red\", alpha=0.3, label=\"Predicted\" if i == 0 else \"\")\n",
    "    plt.title(f\"{TICKER} {FORECAST_HORIZON}-Day Forecast (Validation Samples)\")\n",
    "    plt.xlabel(\"Day Ahead\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def walk_forward_eval(df: pd.DataFrame, n_splits: int = 3):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    dir_scores, mae_scores, rmse_scores = [], [], []\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        train_df = df.iloc[train_idx]\n",
    "        test_df = df.iloc[test_idx[0] - LOOKBACK : test_idx[-1] + 1]\n",
    "        train_scaled = MinMaxScaler().fit_transform(train_df[feature_cols])\n",
    "        price_scaler = MinMaxScaler().fit(train_df[[\"Close\"]])\n",
    "        test_scaled = MinMaxScaler().fit(train_df[feature_cols]).transform(test_df[feature_cols])\n",
    "        X_tr, y_dir_tr, y_fore_tr = create_sequences(\n",
    "            train_scaled, train_df.reset_index(drop=True), LOOKBACK, FORECAST_HORIZON, PREDICT_RETURNS, price_scaler\n",
    "        )\n",
    "        X_te, y_dir_te, y_fore_te = create_sequences(\n",
    "            test_scaled, test_df.reset_index(drop=True), LOOKBACK, FORECAST_HORIZON, PREDICT_RETURNS, price_scaler\n",
    "        )\n",
    "        model = build_model(X_tr.shape[2])\n",
    "        model = compile_model(model)\n",
    "        history = model.fit(\n",
    "            X_tr, {\"direction\": y_dir_tr, \"forecast\": y_fore_tr},\n",
    "            epochs=min(50, EPOCHS), batch_size=BATCH_SIZE, verbose=0,\n",
    "        )\n",
    "        dir_pred, fore_pred = model.predict(X_te, verbose=0)\n",
    "        dir_bin = (dir_pred > 0.5).astype(int)\n",
    "        dir_scores.append(accuracy_score(y_dir_te, dir_bin))\n",
    "        preds, trues = [], []\n",
    "        for i in range(len(fore_pred)):\n",
    "            start_price = test_df[\"Close\"].iloc[i + LOOKBACK - 1]\n",
    "            if PREDICT_RETURNS:\n",
    "                preds.append(logrets_to_prices(start_price, fore_pred[i]))\n",
    "                trues.append(logrets_to_prices(start_price, y_fore_te[i]))\n",
    "            else:\n",
    "                preds.append(price_scaler.inverse_transform(fore_pred[i].reshape(-1, 1)).flatten())\n",
    "                trues.append(price_scaler.inverse_transform(y_fore_te[i].reshape(-1, 1)).flatten())\n",
    "        preds = np.array(preds)\n",
    "        trues = np.array(trues)\n",
    "        mae_scores.append(mean_absolute_error(trues.flatten(), preds.flatten()))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(trues.flatten(), preds.flatten())))\n",
    "    print(f\"Walk-forward Direction Acc: {np.mean(dir_scores):.3f} ± {np.std(dir_scores):.3f}\")\n",
    "    print(f\"Walk-forward Forecast MAE: {np.mean(mae_scores):.2f} ± {np.std(mae_scores):.2f}\")\n",
    "    print(f\"Walk-forward Forecast RMSE: {np.mean(rmse_scores):.2f} ± {np.std(rmse_scores):.2f}\")\n",
    "\n",
    "pred_prices, true_prices = evaluate(model, X_val, y_dir_val, y_fore_val, price_scaler, val_df)\n",
    "plot_history(history)\n",
    "plot_predictions(pred_prices, true_prices)\n",
    "model.save(\"stock_price_model.keras\")\n",
    "print(\"Model saved to stock_price_model.keras\")\n",
    "\n",
    "if RUN_WALK_FORWARD:\n",
    "    walk_forward_eval(df)\n",
    "    print(\"Walk-forward evaluation completed.\")\n",
    "\n",
    "print(\"Step 7 completed: evaluation and saving finished.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
